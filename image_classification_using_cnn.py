# -*- coding: utf-8 -*-
"""Image classification using CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GdsDjtUs1lmfRgLlZlbqep76-RK2hNgr
"""

!wget https://www.dropbox.com/s/h16vq9rab1itifs/CatDog.zip
! unzip CatDog.zip
! rm CatDog.zip

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
import shutil
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout,Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from skimage.io import imread, imshow
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_dir= "/content/training_set/training_set"
d ={"dogs":0,"cats":0}
for dir in os.listdir(train_dir):
  path = os.path.join(train_dir,dir)
  d[dir]=len(os.listdir(path))

print(d)

imshow("/content/training_set/training_set/cats/cat.10.jpg")

imshow("/content/training_set/training_set/dogs/dog.1.jpg")

dir_name=['train','test','val']
ROOT_DIR ='./data_dir'
if not os.path.exists(ROOT_DIR):
  for d in dir_name:
    os.makedirs(os.path.join(ROOT_DIR,d))

for dir in os.listdir(train_dir):
  de = os.path.join(ROOT_DIR,'train',dir)
  os.mkdir(de)

  for img in np.random.choice(os.listdir(os.path.join(train_dir,dir)),int(len(os.listdir(os.path.join(train_dir,dir)))*.8),replace=False):
    org = os.path.join(train_dir,dir,img)
    dest = os.path.join(de,img)
    shutil.copy(org,dest)
    os.remove(org)

for dir in os.listdir(train_dir):
  de = os.path.join(ROOT_DIR,'val',dir)
  os.mkdir(de)

  for img in np.random.choice(os.listdir(os.path.join(train_dir,dir)),int(len(os.listdir(os.path.join(train_dir,dir)))*.5),replace=False):
    org = os.path.join(train_dir,dir,img)
    dest = os.path.join(de,img)
    shutil.copy(org,dest)
    os.remove(org)

for dir in os.listdir(train_dir):
  de = os.path.join(ROOT_DIR,'test',dir)
  os.mkdir(de)

  for img in np.random.choice(os.listdir(os.path.join(train_dir,dir)),int(len(os.listdir(os.path.join(train_dir,dir)))),replace=False):
    org = os.path.join(train_dir,dir,img)
    dest = os.path.join(de,img)
    shutil.copy(org,dest)
    os.remove(org)

traindatagenerator=ImageDataGenerator(rescale=1/255,rotation_range=0.3,zoom_range=0.3,horizontal_flip=True,vertical_flip=True)
train_data=traindatagenerator.flow_from_directory(directory="/content/data_dir/train",target_size=(224,224),class_mode="binary")

testdatagenerator=ImageDataGenerator(rescale=1/255)
test_data=testdatagenerator.flow_from_directory(directory="/content/data_dir/test",target_size=(224,224),class_mode="binary")

valdatagenerator=ImageDataGenerator(rescale=1/255)
val_data=valdatagenerator.flow_from_directory(directory="/content/data_dir/val",target_size=(224,224),class_mode="binary")

train_data.class_indices

CLASS_ID = {j:i for i,j in train_data.class_indices.items()}

CLASS_ID

model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))

model.add(Dropout(0.4))

model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.4))
model.add(Dense(1, activation='sigmoid'))

model.summary()

tf.keras.utils.plot_model(model)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

es = EarlyStopping(monitor='val_accuracy',min_delta=0.01, patience=7,verbose=1,mode='auto')
mc = ModelCheckpoint(filepath='best_model.h5',monitor='val_accuracy',verbose=1,
                     save_best_only=True,
                     mode='auto')
callback=[es,mc]

hist = model.fit_generator(generator=train_data,
                            validation_data=val_data,
                            validation_steps=16,
                            steps_per_epoch=32,
                            epochs=30,
                            verbose=1,
                            callbacks=callback)

from tensorflow.keras.models import load_model
final_model = load_model("best_model.h5")

from typing import Generator
final_model.evaluate_generator(generator=test_data)

from tensorflow.keras.applications.vgg16 import VGG16
vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
vgg.summary()

vgg.get_config()

for layer in vgg.layers[:15]:
  layer.trainable = False
for layer in vgg.layers[15:]:
  layer.trainable = True

final_layer = vgg.layers[-1]

final_layer.output

x = Flatten()(final_layer.output)
x = Dense(512, activation='relu')(x)
x = Dropout(0.4)(x)
x = Dense(1, activation='sigmoid')(x)

transfer_model = tf.keras.models.Model(inputs=vgg.input, outputs=x)

transfer_model.summary()

"""There are 19925505 number of parameter which is too much so we reduce the number of parameter by applying GlobalMax2D layer"""

from tensorflow.keras.layers import GlobalMaxPooling2D

x = GlobalMaxPooling2D()(final_layer.output)
x = Dense(512, activation='relu')(x)
x = Dropout(0.4)(x)
x = Dense(1, activation='sigmoid')(x)

transfer_model = tf.keras.models.Model(inputs=vgg.input, outputs=x)
transfer_model.summary()

transfer_model.compile(loss='binary_crossentropy',
                       optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9),
                       metrics=['accuracy'])

hist = transfer_model.fit_generator(generator=train_data,
                            validation_data=val_data,
                            validation_steps=16,
                            steps_per_epoch=128,
                            epochs=10,
                            verbose=1,
                            callbacks=callback)

best_model = load_model("best_model.h5")
best_model.evaluate_generator(generator=test_data)

